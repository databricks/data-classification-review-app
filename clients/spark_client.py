from typing import Dict
import pandas as pd
from delta.tables import DeltaTable
from pyspark.sql import functions
import time
from databricks.connect import DatabricksSession
import const
from utils import table_utils


class SparkClient:

    GROUPING_COLUMNS = [
        const.RESULT_TABLE_SCHEMA_NAME_KEY,
        const.RESULT_TABLE_TABLE_NAME_KEY,
        const.SUMMARY_COLUMN_NAME_KEY,
        const.SUMMARY_PII_ENTITY_KEY,
    ]

    def __init__(self, logger):
        self._logger = logger

    @property
    def _spark(self):
        return DatabricksSession.builder.serverless(True).getOrCreate()

    # Refresh cluster with a a retry in case the session expires.
    # The retry will retrigger `DatabricksSession.builder.serverless(True).getOrCreate()`
    # which will create a new spark session
    def refresh_cluster(self, n_intervals):
        attempts = 0
        while (attempts < 2):
            try:
                start_time = time.time()
                self._spark.sql("SELECT 1;")
                elapsed_time = time.time() - start_time
                self._logger.info(
                    f"Refreshing cluster {n_intervals} took {elapsed_time:.2f} seconds"
                )
                return
            except Exception as e:
                attempts += 1
                if ("session_id is no longer usable" in str(e)):
                    self._logger.info("Session expired. Refreshing session.")
                    continue
                else:
                    raise e

    def apply_tags(self, source_table_name, updates_dict: Dict[str, list[str]]):
        """
        Apply tags to source table

        :param source_table_name: The source table name
        :param updates_list: Dictionary defining the updates. Format should be:
            {
                'col1': ['entity1', 'entity2'],
                'col2': ['entity3', 'entity4']
            }

        The tag name will be generated by appending 'system_' to the entity name. For example,
        entity 'email' will be tagged as 'system_email'.
        """
        if not table_utils.is_three_level_name(source_table_name):
            raise ValueError(
                f"Table name {source_table_name} must have the form <catalog_name>.<schema_name>.<table_name>"
            )
        start_time = time.time()

        for column in updates_dict:
            tags = {f"system_{entity}": "" for entity in updates_dict[column]}
            self._set_column_tags(source_table_name, column, tags)
            time.sleep(0.25)

        end_time = time.time()
        elapsed_time = end_time - start_time
        self._logger.info(f"Apply tags took {elapsed_time} seconds")

    def get_datasets(self, source_table_name: str) -> tuple[pd.DataFrame, pd.DataFrame]:
        """
        Returns two DataFrames for a given source table:
        1. To Review Dataset (with review_status == NULL)
        2. Reviewed Dataset (with review_status not NULL)
        """
        if not table_utils.is_three_level_name(source_table_name):
            raise ValueError(
                f"Table name {source_table_name} must have the form <catalog_name>.<schema_name>.<table_name>"
            )
        start_time = time.time()

        catalog, schema, table = source_table_name.split(".")

        # Get the full result table DF
        table_name = self._get_result_table_name(source_table_name)
        df = self._spark.table(table_name)

        # Filter data where schema_name and table_name equals the source_table_name
        df_filtered = df.filter(
            (functions.col(const.RESULT_TABLE_SCHEMA_NAME_KEY) == schema)
            & (functions.col(const.RESULT_TABLE_TABLE_NAME_KEY) == table)
        )

        # We should have one row per (schema, table, column name, pii entity)
        grouping_columns = SparkClient.GROUPING_COLUMNS

        # Create a helper column for sorting
        df_processed = df_filtered.withColumn(
            "review_status_not_null",
            functions.when(
                functions.col(const.RESULT_TABLE_REVIEW_STATUS_KEY).isNotNull(), 1
            ).otherwise(0),
        )

        # Create a struct of the fields we need to retain
        df_processed = df_processed.withColumn(
            "data_struct",
            functions.struct(
                functions.col("review_status_not_null"),
                functions.col(const.RESULT_TABLE_SCAN_ID_KEY),
                functions.col(const.RESULT_TABLE_REVIEW_STATUS_KEY),
                functions.col(const.SUMMARY_RATIONALES_KEY),
                functions.col(const.SUMMARY_SAMPLES_KEY),
            ),
        )

        # Use max_by to get the row with the highest priority per group
        # Priority is defined by review_status_not_null and timestamp
        df_deduped = df_processed.groupBy(*grouping_columns).agg(
            functions.max_by(
                "data_struct",
                functions.struct(
                    functions.col("review_status_not_null"),
                    functions.col(const.RESULT_TABLE_TIMESTAMP_KEY),
                ),
            ).alias("max_struct")
        )

        # Extract the fields from the struct
        df_deduped = df_deduped.select(
            *grouping_columns,
            functions.col(f"max_struct.{const.RESULT_TABLE_SCAN_ID_KEY}").alias(
                const.RESULT_TABLE_SCAN_ID_KEY
            ),
            functions.col(f"max_struct.{const.RESULT_TABLE_REVIEW_STATUS_KEY}").alias(
                const.RESULT_TABLE_REVIEW_STATUS_KEY
            ),
            functions.col(f"max_struct.{const.SUMMARY_RATIONALES_KEY}").alias(
                const.SUMMARY_RATIONALES_KEY
            ),
            functions.col(f"max_struct.{const.SUMMARY_SAMPLES_KEY}").alias(
                const.SUMMARY_SAMPLES_KEY
            ),
        )

        # Split into two DataFrames based on review_status being null
        df_to_review = df_deduped.filter(
            functions.col(const.RESULT_TABLE_REVIEW_STATUS_KEY).isNull()
        ).toPandas()
        df_reviewed = df_deduped.filter(
            functions.col(const.RESULT_TABLE_REVIEW_STATUS_KEY).isNotNull()
        ).toPandas()

        elapsed_time = time.time() - start_time
        self._logger.info(f"Get datasets took {elapsed_time:.2f} seconds")

        return df_to_review, df_reviewed

    def update_review_status(self, source_table_name, updates_list, review_status):
        """
        Updates the result table for a given source table with the provided updates list

        :param source_table_name: The source table name
        :param updates_list: List of dictionaries defining the updates. Format should be:
            [{
                'schema_name': schema,
                'table_name': table,
                'column_name': column,
                'pii_entity': email,
                'scan_id': scan_id
            }]
        :param review_status: The review status to set
        """
        if not table_utils.is_three_level_name(source_table_name):
            raise ValueError(
                f"Table name {source_table_name} must have the form <catalog_name>.<schema_name>.<table_name>"
            )
        start_time = time.time()

        table_name = self._get_result_table_name(source_table_name)
        # Read the main DataFrame
        delta_table = DeltaTable.forName(self._spark, table_name)

        # Prepare the updates DataFrame
        updates_df = self._spark.createDataFrame(updates_list)
        updates_df = updates_df.withColumn(
            const.RESULT_TABLE_REVIEW_STATUS_KEY, functions.lit(review_status)
        )

        # Define merge condition
        merge_condition = f"""
        target.{const.RESULT_TABLE_SCAN_ID_KEY} = source.{const.RESULT_TABLE_SCAN_ID_KEY} AND
        target.{const.RESULT_TABLE_SCHEMA_NAME_KEY} = source.{const.RESULT_TABLE_SCHEMA_NAME_KEY} AND
        target.{const.RESULT_TABLE_TABLE_NAME_KEY} = source.{const.RESULT_TABLE_TABLE_NAME_KEY} AND
        target.{const.SUMMARY_COLUMN_NAME_KEY} = source.{const.SUMMARY_COLUMN_NAME_KEY} AND
        target.{const.SUMMARY_PII_ENTITY_KEY} = source.{const.SUMMARY_PII_ENTITY_KEY}
        """

        # Perform the merge
        delta_table.alias("target").merge(
            updates_df.alias("source"), merge_condition
        ).whenMatchedUpdate(
            set={
                const.RESULT_TABLE_REVIEW_STATUS_KEY: f"source.{const.RESULT_TABLE_REVIEW_STATUS_KEY}"
            }
        ).execute()

        end_time = time.time()
        elapsed_time = end_time - start_time
        self._logger.info(f"Update review status took {elapsed_time} seconds")

    def _get_result_table_name(self, source_table_name: str) -> str:
        catalog, _, _ = source_table_name.split(".")
        return f"{catalog}.{const.RESULT_SCHEMA_NAME}.{const.RESULT_TABLE_NAME}"

    def _set_column_tags(
        self, full_table_name: str, column_name: str, tags: Dict[str, str]
    ):
        """
        Set a column's tags. Noop if tags is empty.
        """
        if not tags:
            return
        formatted_pairs = []
        for key, value in tags.items():
            formatted_pairs.append(f"'{key}' = '{value}'")
        result = "(" + ", ".join(formatted_pairs) + ")"

        sql_string = f"""ALTER TABLE {table_utils.sanitize_multi_level_name(full_table_name.lower())} ALTER COLUMN {table_utils.sanitize_identifier(column_name)} SET TAGS {result}"""
        self._spark.sql(sql_string)
